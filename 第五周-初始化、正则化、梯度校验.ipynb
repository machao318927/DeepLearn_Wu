{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "为了让每部分实验更加独立，故将初始化、正则化、梯度校验三部分进行划分独立",
   "id": "139468b726650a52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 软件包",
   "id": "5ef5db5871518d90"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T00:17:02.878673Z",
     "start_time": "2025-04-10T00:17:01.697547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import scipy.io as sio\n",
    "from sympy.abc import theta"
   ],
   "id": "3f602208c830a1c6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 初始化 init_utils",
   "id": "b337865bb7e0370d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T00:17:04.869097Z",
     "start_time": "2025-04-10T00:17:04.842527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用类的方法来写\n",
    "class init_utils:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.parameters = {}\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # 计算 sigmoid 激活函数\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500) # 防止溢出，避免计算出现 inf 或 nan\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    # 计算 relu 激活函数\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        s = np.maximum(0, z) # np.maximum() 逐元素比较 0 和 z 中的每个元素，返回它们中较大的那个。\n",
    "        return s\n",
    "\n",
    "    # 计算交叉熵损失\n",
    "    @staticmethod\n",
    "    def comput_loss(Y_hat, Y): # Y_hat: 预测值(模型最后一层的输出)，Y: 真实值\n",
    "        m = Y.shape[1]\n",
    "        logprobs = np.multiply(-np.log(Y_hat), Y) + np.multiply(-np.log(1 - Y_hat), 1 - Y)\n",
    "        loss = 1. / m * np.sum(logprobs)\n",
    "        return loss\n",
    "\n",
    "    # 实现三层神经网络的前向传播流程\n",
    "    def forward_propagation(self, X):\n",
    "        W1, b1 = self.parameters['W1'], self.parameters['b1']\n",
    "        W2, b2 = self.parameters['W2'], self.parameters['b2']\n",
    "        W3, b3 = self.parameters['W3'], self.parameters['b3']\n",
    "\n",
    "        z1 = np.dot(W1, X) + b1\n",
    "        a1 = self.relu(z1)\n",
    "        z2 = np.dot(W2, a1) + b2\n",
    "        a2 = self.relu(z2)\n",
    "        z3 = np.dot(W3, a2) + b3\n",
    "        a3 = self.sigmoid(z3)\n",
    "\n",
    "        cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "        return a3, cache\n",
    "\n",
    "    # 实现三层神经网络的反向传播流程\n",
    "    @staticmethod\n",
    "    def backward_propagation(X, Y, cache):\n",
    "        (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "        m = X.shape[1]\n",
    "        dz3 = a3 - Y\n",
    "        dW3 = np.dot(dz3, a2.T)\n",
    "        db3 = np.sum(dz3, axis=1, keepdims=True)\n",
    "\n",
    "        da2 = np.dot(W3.T, dz3)\n",
    "        dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "        dW2 = np.dot(dz2, a1.T)\n",
    "        db2 = np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "        da1 = np.dot(W2.T, dz2)\n",
    "        dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "        dW1 = np.dot(dz1, X.T)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "        gradients = {\n",
    "            \"dZ3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "            \"da2\": da2, \"dZ2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "            \"da1\": da1, \"dZ1\": dz1, \"dW1\": dW1, \"db1\": db1\n",
    "        }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    # 参数更新（梯度下降）\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(L):\n",
    "            self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - learning_rate * gradients[\"dW\" + str(l + 1)]\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - learning_rate * gradients[\"db\" + str(l + 1)]\n",
    "\n",
    "    # 进行预测并计算准确率\n",
    "    def predict(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        p = np.zeros((1, m), dtype=np.int)\n",
    "\n",
    "        # 前向传播\n",
    "        Y_hat, caches = self.forward_propagation(X)\n",
    "\n",
    "        # 第一种写法\n",
    "        for i in range(Y_hat.shape[1]):\n",
    "            if Y_hat[0, i] > 0.5:\n",
    "                p[0, i] = 1\n",
    "            else:\n",
    "                p[0, i] = 0\n",
    "\n",
    "        # 打印预测结果\n",
    "        print(\"Accuracy: \" + str(np.mean((p[0, :] == y[0, :]))))\n",
    "\n",
    "        return p\n",
    "\n",
    "        # 第二种写法\n",
    "        # p[0, :] = (Y_hat > 0.5).astype(int)\n",
    "        # print(\"Accuracy:\", np.mean(p == y))\n",
    "        # return p\n",
    "\n",
    "    # 生成一个带噪声的“圆形”数据集, 这是一个二分类任务，用于验证神经网络的非线性分类能力\n",
    "    @staticmethod\n",
    "    def load_dataset(is_plot=True):\n",
    "        np.random.seed(1) # 设置随机种子为 1，保证每次生成的数据集都是相同的。用于训练集\n",
    "        train_X, train_Y = sklearn.datasets.make_circles(n_samples=300, noise=.05)\n",
    "        \"\"\"\n",
    "        使用 sklearn 中的 make_circles 函数生成一个二分类的同心圆形数据集（即两类数据分布在两个圆上），\n",
    "            train_X：维度是 (300, 2)，即 300 个样本，每个样本是二维点 (x1, x2)\n",
    "            train_Y：维度是 (300,)，每个样本的标签（0 或 1）\n",
    "        \"\"\"\n",
    "        np.random.seed(2) # 重新设置随机种子为 2，以生成 不同于训练集 的测试集。\n",
    "        test_X, test_Y = sklearn.datasets.make_circles(n_samples=100, noise=.05)\n",
    "\n",
    "        # 是否需要绘制\n",
    "        if is_plot:\n",
    "            plt.scatter(train_X[:, 0], train_X[:, 1], c=train_Y, s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "        train_X = train_X.T\n",
    "        train_Y = train_Y.reshape((1, train_Y.shape[0]))\n",
    "        test_X = test_X.T\n",
    "        test_Y = test_Y.reshape((1, test_Y.shape[0]))\n",
    "\n",
    "        return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "    # 可视化分类边界\n",
    "    @staticmethod\n",
    "    def plot_decision_boundary(model, X, y):\n",
    "        # 设置边界\n",
    "        x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "        y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "        h = 0.01\n",
    "\n",
    "        # 生成网格\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        # 预测整个网格的函数值\n",
    "        Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # 绘制图像\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "        plt.ylabel('x2')\n",
    "        plt.xlabel('x1')\n",
    "        plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "        plt.show()\n",
    "\n",
    "    # 辅助绘图函数，预测每个点属于哪一类（用于画边界） 返回：0/1 的布尔数组\n",
    "    def predict_dec(self, X):\n",
    "        # 使用前向传播计算最终输出\n",
    "        Y_hat, cache = self.forward_propagation(X.T) # 这里用了X.T\n",
    "        predictions = (Y_hat > 0.5)\n",
    "        return predictions\n",
    "\n",
    "        \"\"\"\n",
    "            这个函数是为了配合 plot_decision_boundary() 函数使用的，它不会打印准确率，也不返回概率，而是直接将预测值进行 0/1 分类。\n",
    "            在绘制决策边界时，每个网格点坐标都会作为 X 传进来，predict_dec 会判断它属于哪一类（0 或 1），从而用颜色进行区分。\n",
    "        \"\"\"\n",
    "\n",
    "# init_utils.load_dataset(is_plot=True)"
   ],
   "id": "c8b79f82609bc018",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 正则化 reg_utils",
   "id": "e04488907fcf7114"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T00:17:08.241772Z",
     "start_time": "2025-04-10T00:17:08.222487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class reg_utils:\n",
    "    def __init__(self, parameters, learning_rate=0.01, layers_dims = [2, 3, 2, 1]):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.layers_dims = layers_dims\n",
    "        self.parameters = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        s = np.maximum(0, z)\n",
    "        return s\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        np.random.seed(3)\n",
    "        L = len(self.layers_dims) // 2\n",
    "\n",
    "        for l in range(1, L):\n",
    "            self.parameters[\"W\" + str(l)] = np.random.randn(self.layers_dims[l], self.layers_dims[l - 1]) / np.sqrt(self.layers_dims[l - 1])\n",
    "            self.parameters[\"b\" + str(l)] = np.zeros((self.layers_dims[l], 1))\n",
    "\n",
    "            # 断言\n",
    "            assert(self.parameters[\"W\" + str(l)].shape == (self.layers_dims[l], self.layers_dims[l - 1]))\n",
    "            assert(self.parameters[\"b\" + str(l)].shape == (self.layers_dims[l], 1))\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        W1 = self.parameters[\"W1\"]\n",
    "        b1 = self.parameters[\"b1\"]\n",
    "        W2 = self.parameters[\"W2\"]\n",
    "        b2 = self.parameters[\"b2\"]\n",
    "        W3 = self.parameters[\"W3\"]\n",
    "        b3 = self.parameters[\"b3\"]\n",
    "\n",
    "        z1 = np.dot(W1, X) + b1\n",
    "        a1 = self.tanh(z1)\n",
    "        z2 = np.dot(W2, a1) + b2\n",
    "        a2 = self.tanh(z2)\n",
    "        z3 = np.dot(W3, a2) + b3\n",
    "        a3 = self.sigmoid(z3)\n",
    "\n",
    "        cache = (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3)\n",
    "\n",
    "        return a3, cache\n",
    "\n",
    "    @staticmethod\n",
    "    def computer_cost(Y_hat, Y):\n",
    "        m = Y.shape[1]\n",
    "        logprobs = np.multiply(np.log(Y_hat), Y) + np.multiply(np.log(1 - Y_hat), (1 - Y))\n",
    "        cost = - np.sum(logprobs) / m\n",
    "\n",
    "        return cost\n",
    "\n",
    "    @staticmethod\n",
    "    def backward_propagation(X, Y, cache):\n",
    "        (z1, a1, W1, b1, z2, a2, W2, b2, z3, a3, W3, b3) = cache\n",
    "        m = X.shape[1]\n",
    "        dz3 = a3 - Y\n",
    "        dW3 = np.dot(dz3, a2.T)\n",
    "        db3 = np.sum(dz3, axis=1, keepdims=True)\n",
    "\n",
    "        da2 = np.dot(W3.T, dz3)\n",
    "        dz2 = np.multiply(da2, np.int64(a2 > 0))\n",
    "        dW2 = np.dot(dz2, a1.T)\n",
    "        db2 = np.sum(dz2, axis=1, keepdims=True)\n",
    "\n",
    "        da1 = np.dot(W2.T, dz2)\n",
    "        dz1 = np.multiply(da1, np.int64(a1 > 0))\n",
    "        dW1 = np.dot(dz1, X.T)\n",
    "        db1 = np.sum(dz1, axis=1, keepdims=True)\n",
    "\n",
    "        gradients = {\n",
    "            \"dZ3\": dz3, \"dW3\": dW3, \"db3\": db3,\n",
    "            \"da2\": da2, \"dZ2\": dz2, \"dW2\": dW2, \"db2\": db2,\n",
    "            \"da1\": da1, \"dZ1\": dz1, \"dW1\": dW1, \"db1\": db1\n",
    "        }\n",
    "\n",
    "        return gradients\n",
    "\n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(L):\n",
    "            self.parameters[\"W\" + str(l + 1)] = self.parameters[\"W\" + str(l + 1)] - learning_rate * gradients[\"dW\" + str(l + 1)]\n",
    "            self.parameters[\"b\" + str(l + 1)] = self.parameters[\"b\" + str(l + 1)] - learning_rate * gradients[\"db\" + str(l + 1)]\n",
    "\n",
    "    # 加载数据集\n",
    "    @staticmethod\n",
    "    def load_2D_dataset(is_plot=False):\n",
    "        data = sio.loadmat('datasets/data.mat')\n",
    "        train_X = data['X'].T\n",
    "        train_Y = data['y'].T\n",
    "        test_X = data['Xval'].T\n",
    "        test_Y = data['yval'].T\n",
    "        if is_plot:\n",
    "            plt.scatter(train_X[0, :], train_X[1, :], c=train_Y, s=40, cmap=plt.cm.Spectral)\n",
    "        return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        p = np.zeros((1, m), dtype=np.int)\n",
    "\n",
    "        # 前向传播\n",
    "        Y_hat, caches = self.forward_propagation(X)\n",
    "\n",
    "        # 第一种写法\n",
    "        for i in range(Y_hat.shape[1]):\n",
    "            if Y_hat[0, i] > 0.5:\n",
    "                p[0, i] = 1\n",
    "            else:\n",
    "                p[0, i] = 0\n",
    "\n",
    "        # 打印预测结果\n",
    "        print(\"Accuracy: \" + str(np.mean((p[0, :] == y[0, :]))))\n",
    "\n",
    "        return p\n",
    "\n",
    "        # 第二种写法\n",
    "        # p[0, :] = (Y_hat > 0.5).astype(int)\n",
    "        # print(\"Accuracy:\", np.mean(p == y))\n",
    "        # return p\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_decision_boundary(model, X, y):\n",
    "        # 设置边界\n",
    "        x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "        y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "        h = 0.01\n",
    "\n",
    "        # 生成网格\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        # 预测整个网格的函数值\n",
    "        Z = model(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        # 绘制图像\n",
    "        plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "        plt.ylabel('x2')\n",
    "        plt.xlabel('x1')\n",
    "        plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)\n",
    "        plt.show()\n",
    "\n",
    "    # 辅助绘图函数，预测每个点属于哪一类（用于画边界） 返回：0/1 的布尔数组\n",
    "    def predict_dec(self, X):\n",
    "        # 使用前向传播计算最终输出\n",
    "        Y_hat, cache = self.forward_propagation(X.T) # 这里用了X.T\n",
    "        predictions = (Y_hat > 0.5)\n",
    "        return predictions\n",
    "\n",
    "# reg_utils.load_2D_dataset(is_plot=True)\n"
   ],
   "id": "96804b5448021216",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 梯度检验 gc_utils",
   "id": "242532a7d0496cf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T07:25:11.054827Z",
     "start_time": "2025-04-10T07:25:11.045035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class gc_utils:\n",
    "    def __init__(self, parameters):\n",
    "        self.parameters = parameters\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        return s\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        s = np.maximum(0, z)\n",
    "        return s\n",
    "\n",
    "    # 字典与向量转换函数\n",
    "    # 计算“数值梯度”时要对整个参数向量进行扰动，这个函数完成“展平”。\n",
    "    def dictionary_to_vector(self):\n",
    "        \"\"\"\n",
    "        你有一个三层的神经网络：\n",
    "            第 1 层：输入 4 个神经元 → 输出 5 个神经元，所以：\n",
    "                W1：shape = (5, 4)\n",
    "                b1：shape = (5, 1)\n",
    "            第 2 层：5 → 3\n",
    "                W2：shape = (3, 5)\n",
    "                b2：shape = (3, 1)\n",
    "            第 3 层：3 → 1\n",
    "                W3：shape = (1, 3)\n",
    "                b3：shape = (1, 1)\n",
    "            所以这个神经网络总参数量 = 5×4 + 5 + 3×5 + 3 + 1×3 + 1 = 47 个元素。\n",
    "        \"\"\"\n",
    "        # 将参数字典（每层的权重和偏置）拉平成一个大向量，便于数值梯度计算时统一处理。\n",
    "        keys = [] # 保存参数来源的键（例如：W1、b1），方便调试。\n",
    "        count = 0\n",
    "        for key in ['W1', 'b1', 'W2', 'b2', 'W3', 'b3']:\n",
    "            new_vector = np.reshape(self.parameters[key], (-1, 1))\n",
    "            keys = keys + [key] * new_vector.shape[0]\n",
    "\n",
    "            if count == 0:\n",
    "                theta = new_vector # 如果是第一个参数，就直接赋值给 theta，因为这时候还没有已有的列向量可以拼接。\n",
    "            else:\n",
    "                theta = np.concatenate((theta, new_vector), axis=0) # 把当前这个参数 new_vector 沿着第0轴（也就是“竖着”）拼接到原有的 theta 后面，形成一个更长的列向量。\n",
    "            count += 1\n",
    "\n",
    "        return theta, keys\n",
    "\n",
    "    def vector_to_dictionary(self, theta):\n",
    "\n",
    "        \"\"\"\n",
    "            作用：把拉平（flatten）成向量的参数 theta，重新还原成一个结构化的字典形式，包含各层的权重（W）和偏置（b）。\n",
    "            梯度检验过程中，将数值梯度 theta 还原成可用于前向/反向传播的参数形式。\n",
    "\n",
    "        :param theta:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.parameters = {}  # 重新构造字典\n",
    "        self.parameters[\"W1\"] = theta[:20].reshape((5, 4))\n",
    "        self.parameters[\"b1\"] = theta[20:25].reshape((5, 1))\n",
    "        self.parameters[\"W2\"] = theta[25:40].reshape((3, 5))\n",
    "        self.parameters[\"b2\"] = theta[40:43].reshape((3, 1))\n",
    "        self.parameters[\"W3\"] = theta[43:46].reshape((1, 3))\n",
    "        self.parameters[\"b3\"] = theta[46:47].reshape((1, 1))\n",
    "\n",
    "        return self.parameters\n",
    "\n",
    "    # def test_dictionary_to_vector(self):\n",
    "    #     theta, keys = self.dictionary_to_vector()\n",
    "    #     print(\"✅ 测试 dictionary_to_vector()\")\n",
    "    #     print(\"theta shape:\", theta.shape)\n",
    "    #     print(\"theta:\\n\", theta)\n",
    "    #     print(\"\\nkeys length:\", len(keys))\n",
    "    #     print(\"keys:\\n\", keys)\n",
    "    #     return theta  # 注意：我们返回 theta 以供下一步测试用\n",
    "    #\n",
    "    # def test_vector_to_dictionary(self, theta):\n",
    "    #     parameters = self.vector_to_dictionary(theta)\n",
    "    #     print(\"\\n✅ 测试 vector_to_dictionary()\")\n",
    "    #     for key in parameters:\n",
    "    #         print(f\"{key}: shape = {parameters[key].shape}\")\n",
    "    #         print(parameters[key], \"\\n\")\n",
    "\n",
    "    def gradients_to_vector(self, gradients):\n",
    "        \"\"\"\n",
    "        将梯度字典（每层的权重和偏置的导数）展开成一个列向量，用于梯度检验。\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        for key in ['dW1', 'db1', 'dW2', 'db2', 'dW3', 'db3']:\n",
    "            new_vector = np.reshape(gradients[key], (-1, 1))\n",
    "\n",
    "            if count == 0:\n",
    "                theta = new_vector\n",
    "            else:\n",
    "                theta = np.concatenate((theta, new_vector), axis=0)\n",
    "            count = count + 1\n",
    "        return theta\n",
    "\n",
    "    # def test_gradients_to_verctor(self, gradients):\n",
    "    #     theta = self.gradients_to_vector(gradients)\n",
    "    #     print(\"✅ 测试 gradients_to_vector()\")\n",
    "    #     print(\"theta shape:\", theta.shape)\n",
    "    #     print(\"theta:\\n\", theta)\n",
    "\n",
    "\n",
    "# # 测试\n",
    "# parameters = {\n",
    "#     \"W1\": np.random.randn(5, 4),\n",
    "#     \"b1\": np.random.randn(5, 1),\n",
    "#     \"W2\": np.random.randn(3, 5),\n",
    "#     \"b2\": np.random.randn(3, 1),\n",
    "#     \"W3\": np.random.randn(1, 3),\n",
    "#     \"b3\": np.random.randn(1, 1)\n",
    "# }\n",
    "# gradients = {\n",
    "#     \"dW1\": np.random.randn(5, 4),\n",
    "#     \"db1\": np.random.randn(5, 1),\n",
    "#     \"dW2\": np.random.randn(3, 5),\n",
    "#     \"db2\": np.random.randn(3, 1),\n",
    "#     \"dW3\": np.random.randn(1, 3),\n",
    "#     \"db3\": np.random.randn(1, 1)\n",
    "# }\n",
    "# \n",
    "# # 实例化对象\n",
    "# utils = gc_utils(parameters)\n",
    "\n",
    "# 执行测试\n",
    "# theta = utils.test_dictionary_to_vector()\n",
    "# utils.test_vector_to_dictionary(theta)\n",
    "# utils.test_gradients_to_verctor(gradients)\n",
    "\n"
   ],
   "id": "24e02e8e99328ea0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 测试 gradients_to_vector()\n",
      "theta shape: (47, 1)\n",
      "theta:\n",
      " [[ 0.26420726]\n",
      " [-1.38781385]\n",
      " [-0.34017428]\n",
      " [-2.77613197]\n",
      " [-0.94402699]\n",
      " [ 0.46871342]\n",
      " [-0.57081029]\n",
      " [-0.28589654]\n",
      " [ 0.66540024]\n",
      " [-0.30886063]\n",
      " [-0.2374728 ]\n",
      " [ 1.54074145]\n",
      " [-1.05898817]\n",
      " [-0.56024939]\n",
      " [ 0.34294975]\n",
      " [-0.02619994]\n",
      " [-0.37552326]\n",
      " [ 0.41785468]\n",
      " [ 0.07723412]\n",
      " [-0.17817048]\n",
      " [ 1.24579233]\n",
      " [ 0.32770042]\n",
      " [-0.61012564]\n",
      " [-0.79818856]\n",
      " [ 0.77278608]\n",
      " [ 1.25692487]\n",
      " [ 2.21832917]\n",
      " [-0.33087184]\n",
      " [ 1.86246082]\n",
      " [-0.17362849]\n",
      " [-0.62221911]\n",
      " [-0.0276552 ]\n",
      " [-1.2857421 ]\n",
      " [-1.09168258]\n",
      " [-1.62613926]\n",
      " [-0.3150761 ]\n",
      " [ 0.07025188]\n",
      " [ 0.00905691]\n",
      " [ 0.22080611]\n",
      " [-0.08227922]\n",
      " [ 0.09129198]\n",
      " [ 0.06030096]\n",
      " [ 0.33770794]\n",
      " [ 0.28489654]\n",
      " [-0.19556954]\n",
      " [-0.84500725]\n",
      " [ 0.81312262]]\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
