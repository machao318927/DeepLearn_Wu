{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "构建两个神经网络，一个是构建两层的神经网络，一个是构建多层的神经网络，多层神经网络的层数可以自己定义。本次的教程的难度有所提升，但是我会力求深入简出。在这里，我们简单的讲一下难点，本文会提到`LINEAR-> ACTIVATION`转发函数，比如我有一个多层的神经网络，结构是`输入层->隐藏层->隐藏层->···->隐藏层->输出层`，在每一层中，我会首先计算`Z = np.dot(W,A) + b`，这叫做`linear_forward`，然后再计算`A = relu(Z)` 或者 `A = sigmoid(Z)`，这叫做`linear_activation_forward`，合并起来就是这一层的计算方法，所以每一层的计算都有两个步骤，先是计算Z，再计算A\n",
    "\n",
    "<img src=\"./picture/img_4.png\" width=\"70%\">"
   ],
   "id": "275bdfa27ba411ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "步骤：\n",
    "\n",
    "1. 初始化网络参数\n",
    "\n",
    "2. 前向传播\n",
    "\n",
    "    1. 计算一层的中线性求和的部分\n",
    "\n",
    "    2. 计算激活函数的部分（ReLU使用L-1次，Sigmod使用1次）\n",
    "\n",
    "    3. 结合线性求和与激活函数\n",
    "\n",
    "3. 计算误差\n",
    "\n",
    "4. 反向传播\n",
    "\n",
    "    1. 线性部分的反向传播公式\n",
    "\n",
    "    2. 激活函数部分的反向传播公式\n",
    "\n",
    "    3. 结合线性部分与激活函数的反向传播公式\n",
    "\n",
    "5. 更新参数"
   ],
   "id": "2f00bb9d7f79df17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 准备软件包",
   "id": "396a47981f360a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:36:07.328383Z",
     "start_time": "2025-04-06T13:36:07.320196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ],
   "id": "5ffbb4cfd8908cc1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 数据加载函数",
   "id": "392b495cf8ea9eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:18:56.398598Z",
     "start_time": "2025-04-06T13:18:56.390360Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_dataset():\n",
    "\n",
    "    # 读取训练集\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'][:]) # 加 [:] 是把数据从 h5py.Dataset 转换为 numpy.ndarray;外层再用 np.array() 是为了确保类型一致（有时是冗余的，但保险）。\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'][:])\n",
    "\n",
    "    # 读取测试集\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'][:])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'][:])\n",
    "\n",
    "    # 获取类别标签\n",
    "    classes = np.array(test_dataset['list_classes'][:])\n",
    "\n",
    "    # 标签 reshape 处理\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) # 原始标签形状是 (m,)，现在 reshape 成 (1, m)，很多神经网络实现中要求标签是形如 (1, m) 的二维数组\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # 返回所有数据\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "4df01b8cca2389a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 常见激活函数",
   "id": "4eaa22eb891529f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:18:59.384693Z",
     "start_time": "2025-04-06T13:18:59.376033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    实现 Sigmoid 激活函数（用 NumPy 实现）\n",
    "\n",
    "    Arguments:\n",
    "    Z -- 任意形状的 numpy 数组\n",
    "\n",
    "    Returns:\n",
    "    A -- sigmoid(Z) 的输出，形状与 Z 相同\n",
    "    cache -- 返回 Z，用于反向传播时使用\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    实现单个 Sigmoid 单元的反向传播\n",
    "\n",
    "    Arguments:\n",
    "    dA -- 激活后的梯度，任意形状\n",
    "\n",
    "    cache -- 前向传播时保存的 Z，用于高效地计算反向传播\n",
    "\n",
    "    Returns:\n",
    "    dZ -- 成本对 Z 的梯度\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    实现 ReLU 激活函数\n",
    "\n",
    "    Arguments:\n",
    "    Z -- 线性层的输出，任意形状\n",
    "\n",
    "    Returns:\n",
    "    A -- 激活后的输出，形状与 Z 相同\n",
    "\n",
    "    cache -- 一个包含 Z 的变量，用于高效地进行反向传播计算\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    实现单个 ReLU 单元的反向传播\n",
    "\n",
    "    Arguments:\n",
    "    dA -- 激活后的梯度，任意形状\n",
    "\n",
    "    cache -- 前向传播时保存的 Z，用于高效地计算反向传播\n",
    "\n",
    "    Returns:\n",
    "    dZ -- 成本对 Z 的梯度\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ],
   "id": "2f017b8c8951b477",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- testCases：提供了一些测试示例来评估函数的正确性，参见下载的资料或者在底部查看它的代码。\n",
    "\n",
    "每个函数都用来模拟某个阶段的输入和输出，以便你在实现具体算法时可以用这些数据来验证自己代码是否正确。"
   ],
   "id": "c6162106366fe78c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:03:24.838576Z",
     "start_time": "2025-04-06T14:03:24.827567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造用于测试线性前向传播的输入：输入激活值 A、权重矩阵 W 和偏置项 b。\n",
    "def linear_forward_test_case():\n",
    "    np.random.seed(1)\n",
    "    A = np.random.randn(3,2) # 随机生成一个 3x2 的矩阵 A\n",
    "    W = np.random.randn(1,3) # 随机生成一个 1x3 的矩阵 W\n",
    "    b = np.random.randn(1,1) # 随机生成一个 1x1 的矩阵 b\n",
    "\n",
    "    return A,W,b\n",
    "\n",
    "# 构造用于测试“线性计算+激活函数”的输入参数\n",
    "def linear_activation_forward_test_case():\n",
    "    np.random.seed(2)\n",
    "    A_prev = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "\n",
    "    return A_prev, W, b\n",
    "\n",
    "# 生成一个 2 层神经网络的参数和输入 X，用于测试整个 L 层前向传播函数。\n",
    "def L_model_forward_test_case():\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return X, parameters\n",
    "\n",
    "# 生成用于测试损失函数（交叉熵）计算的预测值 aL 和真实标签 Y。\n",
    "def compute_cost_test_case():\n",
    "    Y = np.array([[1, 1, 1]]) # 三个样本的真实标签，全是1\n",
    "    aL = np.array([[.8, .9, 0.4]]) # 三个样本的预测值，也叫“激活值”，是神经网络的输出\n",
    "\n",
    "    return Y, aL\n",
    "\n",
    "# 用于测试线性部分的反向传播函数。\n",
    "def linear_backward_test_case():\n",
    "    np.random.seed(1)\n",
    "    dZ = np.random.randn(1,2)\n",
    "    A = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "\n",
    "    linear_cache = (A, W, b)\n",
    "\n",
    "    return dZ, linear_cache\n",
    "\n",
    "# 用于测试带激活函数的反向传播。\n",
    "def linear_activation_backward_test_case():\n",
    "    np.random.seed(2)\n",
    "    dA = np.random.randn(1,2) # 当前层的激活梯度 dA，1x2\n",
    "    A = np.random.randn(3,2) # 前一层的激活值，3x2\n",
    "    W = np.random.randn(1,3) # 权重矩阵，1x3\n",
    "    b = np.random.randn(1,1) # 偏置项，1x1\n",
    "    Z = np.dot(W.T, A) + b # 当前层的 Z（线性输出）\n",
    "\n",
    "    activation_cache = (Z) # 激活函数缓存\n",
    "\n",
    "    linear_cache = (A, W, b) # 前向传播缓存\n",
    "    linear_activation_cache = (linear_cache, activation_cache) # 合并缓存\n",
    "\n",
    "    return dA, linear_activation_cache\n",
    "\n",
    "# 生成完整神经网络的反向传播输入，包括前一层和后一层的 cache。\n",
    "def L_model_backward_test_case():\n",
    "    np.random.seed(3)\n",
    "    AL = np.random.randn(1,2) # 最后一层的输出预测值\n",
    "    Y = np.array([[1, 0]]) # 真实标签，只有两个样本\n",
    "\n",
    "    A1 = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    Z1 = np.random.randn(3,2)\n",
    "    linear_cache_activation_1 = ((A1, W1, b1), Z1) # 第一层缓存\n",
    "\n",
    "    A2 = np.random.randn(3, 2)\n",
    "    W2 = np.random.randn(1, 3)\n",
    "    b2 = np.random.randn(1, 1)\n",
    "    Z2 = np.random.randn(1, 2)\n",
    "    linear_cache_activation_2 = ((A2, W2, b2), Z2) # 第二层缓存\n",
    "\n",
    "    caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "    return AL, Y, caches\n",
    "\n",
    "# 生成测试用的参数和梯度，用于测试参数更新函数。\n",
    "def update_parameters_test_case():\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    np.random.seed(3)\n",
    "    dW1 = np.random.randn(3,4)\n",
    "    db1 = np.random.randn(3,1)\n",
    "    dW2 = np.random.randn(1,3)\n",
    "    db2 = np.random.randn(1,1)\n",
    "\n",
    "    grads = {\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1,\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2\n",
    "    }\n",
    "\n",
    "    return parameters, grads"
   ],
   "id": "d282a30ce4aa9575",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 初始化参数\n",
    "### （1）对于一个两层的神经网络结构而言，模型结构是线性->ReLU->线性->sigmod函数。"
   ],
   "id": "ebd0f253875c1bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T13:39:07.041323Z",
     "start_time": "2025-04-06T13:39:07.022598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    此函数是为了初始化两层网络参数而使用的函数。\n",
    "    参数：\n",
    "        n_x - 输入层节点数量\n",
    "        n_h - 隐藏层节点数量\n",
    "        n_y - 输出层节点数量\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含你的参数的python字典：\n",
    "            W1 - 权重矩阵,维度为（n_h，n_x）\n",
    "            b1 - 偏向量，维度为（n_h，1）\n",
    "            W2 - 权重矩阵，维度为（n_y，n_h）\n",
    "            b2 - 偏向量，维度为（n_y，1）\n",
    "\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    b2 = np.random.randn(n_y, 1)\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# 测试\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ],
   "id": "2c55a985a094283e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.02060141 -0.00322417 -0.00384054]\n",
      " [ 0.01133769 -0.01099891 -0.00172428]]\n",
      "b1 = [[ 0.58281521]\n",
      " [-1.10061918]]\n",
      "W2 = [[-0.00877858  0.00042214]]\n",
      "b2 = [[1.14472371]]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### （2）初始化多层网络参数",
   "id": "44f9bfa343cec5a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:00:52.829359Z",
     "start_time": "2025-04-06T14:00:52.823357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    此函数是为了初始化多层网络参数而使用的函数。\n",
    "    参数：\n",
    "        layers_dims - 包含我们网络中每个图层的节点数量的列表\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含参数“W1”，“b1”，...，“WL”，“bL”的字典：\n",
    "                     W1 - 权重矩阵，维度为（layers_dims [1]，layers_dims [1-1]）\n",
    "                     bl - 偏向量，维度为（layers_dims [1]，1）\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L): # 输入层定义为0层，故从1开始\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) # Xavier 初始化, 缓解梯度爆炸和梯度消失\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        # 断言\n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# 测试\n",
    "layer_dims = [5,4,3,2]\n",
    "print(len(layer_dims))\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ],
   "id": "2aa6a5eebf2802b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "W1 = [[ 0.79989897  0.19521314  0.04315498 -0.83337927 -0.12405178]\n",
      " [-0.15865304 -0.03700312 -0.28040323 -0.01959608 -0.21341839]\n",
      " [-0.58757818  0.39561516  0.39413741  0.76454432  0.02237573]\n",
      " [-0.18097724 -0.24389238 -0.69160568  0.43932807 -0.49241241]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.59252326 -0.10282495  0.74307418  0.11835813]\n",
      " [-0.51189257 -0.3564966   0.31262248 -0.08025668]\n",
      " [-0.38441818 -0.11501536  0.37252813  0.98805539]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[-0.71829494 -0.36166197 -0.46405457]\n",
      " [-1.39665832 -0.53335157 -0.59113495]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 我们分别构建了两层和多层神经网络的初始化参数的函数，现在我们开始构建前向传播函数。",
   "id": "f329b49a6f949c88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 前向传播函数\n",
    "前向传播有以下三个步骤:\n",
    "\n",
    "1. LINEAR\n",
    "2. LINEAR - >ACTIVATION，其中激活函数将会使用ReLU或Sigmoid。\n",
    "3. [LINEAR - > RELU] ×（L-1） - > LINEAR - > SIGMOID（整个模型）\n",
    "\n",
    "线性正向传播模块（向量化所有示例）使用公式(3)进行计算：\n",
    "\n",
    "<img src=\"./picture/img_5.png\" width=\"30%\">\n"
   ],
   "id": "a6f6b3514861728e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性部分【LINEAR】\n",
    "前向传播中，线性部分计算如下："
   ],
   "id": "5db4da5cc8c00a28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:04:38.276080Z",
     "start_time": "2025-04-06T14:04:38.262104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forword(A, W, b):\n",
    "    \"\"\"\n",
    "    实现前向传播的线性部分。\n",
    "\n",
    "    参数：\n",
    "        A - 来自上一层（或输入数据）的激活，维度为(上一层的节点数量，示例的数量）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前图层的节点数量，前一图层的节点数量）\n",
    "        b - 偏向量，numpy向量，维度为（当前图层节点数量，1）\n",
    "\n",
    "    返回：\n",
    "         Z - 激活功能的输入，也称为预激活参数\n",
    "         cache - 一个包含“A”，“W”和“b”的字典，存储这些变量以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "\n",
    "    cache = (A, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "# 测试\n",
    "A, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forword(A, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "print(\"linear_cache = \" + str(linear_cache))\n"
   ],
   "id": "6eab1c2799682ac3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n",
      "linear_cache = (array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]]))\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "前向传播的单层计算完成了一半啦！我们来开始构建后半部分",
   "id": "6ff6e1628c7cfec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性激活部分【LINEAR - >ACTIVATION】\n",
    "为了更方便，我们将把两个功能（线性和激活）分组为一个功能（LINEAR-> ACTIVATION）。 因此，我们将实现一个执行LINEAR前进步骤，然后执行ACTIVATION前进步骤的功能。我们来看看这激活函数的数学实现吧~\n",
    "\n",
    "<img src=\"./picture/img_6.png\" width=\"50%\">"
   ],
   "id": "c27ec1527c7cc34a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-06T14:10:11.719235Z",
     "start_time": "2025-04-06T14:10:11.707036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    实现LINEAR-> ACTIVATION 这一层的前向传播\n",
    "\n",
    "    参数：\n",
    "        A_prev - 来自上一层（或输入层）的激活，维度为(上一层的节点数量，示例数）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前层的节点数量，前一层的大小）\n",
    "        b - 偏向量，numpy阵列，维度为（当前层的节点数量，1）\n",
    "        activation - 选择在此层中使用的激活函数名，字符串类型，【\"sigmoid\" | \"relu\"】\n",
    "\n",
    "    返回：\n",
    "        A - 激活函数的输出，也称为激活后的值\n",
    "        cache - 一个包含“linear_cache”和“activation_cache”的字典，我们需要存储它以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forword(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forword(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "# 测试\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ],
   "id": "9cb318ee09e80526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
