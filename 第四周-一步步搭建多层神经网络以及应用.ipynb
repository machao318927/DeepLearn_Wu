{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "构建两个神经网络，一个是构建两层的神经网络，一个是构建多层的神经网络，多层神经网络的层数可以自己定义。本次的教程的难度有所提升，但是我会力求深入简出。在这里，我们简单的讲一下难点，本文会提到`LINEAR-> ACTIVATION`转发函数，比如我有一个多层的神经网络，结构是`输入层->隐藏层->隐藏层->···->隐藏层->输出层`，在每一层中，我会首先计算`Z = np.dot(W,A) + b`，这叫做`linear_forward`，然后再计算`A = relu(Z)` 或者 `A = sigmoid(Z)`，这叫做`linear_activation_forward`，合并起来就是这一层的计算方法，所以每一层的计算都有两个步骤，先是计算Z，再计算A\n",
    "\n",
    "<img src=\"./picture/img_4.png\" width=\"70%\">"
   ],
   "id": "275bdfa27ba411ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "步骤：\n",
    "\n",
    "1. 初始化网络参数\n",
    "\n",
    "2. 前向传播\n",
    "\n",
    "    1. 计算一层的中线性求和的部分\n",
    "\n",
    "    2. 计算激活函数的部分（ReLU使用L-1次，Sigmod使用1次）\n",
    "\n",
    "    3. 结合线性求和与激活函数\n",
    "\n",
    "3. 计算误差\n",
    "\n",
    "4. 反向传播\n",
    "\n",
    "    1. 线性部分的反向传播公式\n",
    "\n",
    "    2. 激活函数部分的反向传播公式\n",
    "\n",
    "    3. 结合线性部分与激活函数的反向传播公式\n",
    "\n",
    "5. 更新参数"
   ],
   "id": "2f00bb9d7f79df17"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 准备软件包",
   "id": "396a47981f360a9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:50:47.034364Z",
     "start_time": "2025-04-07T06:50:46.454935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1)"
   ],
   "id": "5ffbb4cfd8908cc1",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 数据加载函数",
   "id": "392b495cf8ea9eb3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:50:47.050219Z",
     "start_time": "2025-04-07T06:50:47.040065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_dataset():\n",
    "\n",
    "    # 读取训练集\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset['train_set_x'][:]) # 加 [:] 是把数据从 h5py.Dataset 转换为 numpy.ndarray;外层再用 np.array() 是为了确保类型一致（有时是冗余的，但保险）。\n",
    "    train_set_y_orig = np.array(train_dataset['train_set_y'][:])\n",
    "\n",
    "    # 读取测试集\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset['test_set_x'][:])\n",
    "    test_set_y_orig = np.array(test_dataset['test_set_y'][:])\n",
    "\n",
    "    # 获取类别标签\n",
    "    classes = np.array(test_dataset['list_classes'][:])\n",
    "\n",
    "    # 标签 reshape 处理\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0])) # 原始标签形状是 (m,)，现在 reshape 成 (1, m)，很多神经网络实现中要求标签是形如 (1, m) 的二维数组\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "\n",
    "    # 返回所有数据\n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ],
   "id": "4df01b8cca2389a7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 常见激活函数",
   "id": "4eaa22eb891529f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:50:49.282738Z",
     "start_time": "2025-04-07T06:50:49.274337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    实现 Sigmoid 激活函数（用 NumPy 实现）\n",
    "\n",
    "    Arguments:\n",
    "    Z -- 任意形状的 numpy 数组\n",
    "\n",
    "    Returns:\n",
    "    A -- sigmoid(Z) 的输出，形状与 Z 相同\n",
    "    cache -- 返回 Z，用于反向传播时使用\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    实现单个 Sigmoid 单元的反向传播\n",
    "\n",
    "    Arguments:\n",
    "    dA -- 激活后的梯度，任意形状\n",
    "\n",
    "    cache -- 前向传播时保存的 Z，用于高效地计算反向传播\n",
    "\n",
    "    Returns:\n",
    "    dZ -- 成本对 Z 的梯度\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    实现 ReLU 激活函数\n",
    "\n",
    "    Arguments:\n",
    "    Z -- 线性层的输出，任意形状\n",
    "\n",
    "    Returns:\n",
    "    A -- 激活后的输出，形状与 Z 相同\n",
    "\n",
    "    cache -- 一个包含 Z 的变量，用于高效地进行反向传播计算\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    实现单个 ReLU 单元的反向传播\n",
    "\n",
    "    Arguments:\n",
    "    dA -- 激活后的梯度，任意形状\n",
    "\n",
    "    cache -- 前向传播时保存的 Z，用于高效地计算反向传播\n",
    "\n",
    "    Returns:\n",
    "    dZ -- 成本对 Z 的梯度\n",
    "    \"\"\"\n",
    "\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) \n",
    "\n",
    "    dZ[Z <= 0] = 0\n",
    "\n",
    "    assert (dZ.shape == Z.shape)\n",
    "\n",
    "    return dZ\n"
   ],
   "id": "2f017b8c8951b477",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- testCases：提供了一些测试示例来评估函数的正确性，参见下载的资料或者在底部查看它的代码。\n",
    "\n",
    "每个函数都用来模拟某个阶段的输入和输出，以便你在实现具体算法时可以用这些数据来验证自己代码是否正确。"
   ],
   "id": "c6162106366fe78c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:57:27.984512Z",
     "start_time": "2025-04-07T07:57:27.967342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 构造用于测试线性前向传播的输入：输入激活值 A、权重矩阵 W 和偏置项 b。\n",
    "def linear_forward_test_case():\n",
    "    np.random.seed(1)\n",
    "    A_prev = np.random.randn(3,2) # 随机生成一个 3x2 的矩阵 A （前一层）\n",
    "    W = np.random.randn(1,3) # 随机生成一个 1x3 的矩阵 W （当前层）\n",
    "    b = np.random.randn(1,1) # 随机生成一个 1x1 的矩阵 b （当前层）\n",
    "\n",
    "    return A_prev,W,b\n",
    "\n",
    "# 构造用于测试“线性计算+激活函数”的输入参数·\n",
    "def linear_activation_forward_test_case():\n",
    "    # 前一层：三个神经元，当前层：一个神经元\n",
    "    np.random.seed(2)\n",
    "    A_prev = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "\n",
    "    return A_prev, W, b\n",
    "\n",
    "# 生成一个 2 层神经网络的参数和输入 X，用于测试整个 L 层前向传播函数。\n",
    "def L_model_forward_test_case():\n",
    "    np.random.seed(1)\n",
    "    X = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return X, parameters\n",
    "\n",
    "# 生成用于测试损失函数（交叉熵）计算的预测值 aL 和真实标签 Y。\n",
    "def compute_cost_test_case():\n",
    "    Y = np.array([[1, 1, 1]]) # 三个样本的真实标签，全是1\n",
    "    aL = np.array([[.8, .9, 0.4]]) # 三个样本的预测值，也叫“激活值”，是神经网络的输出\n",
    "\n",
    "    return Y, aL\n",
    "\n",
    "# 用于测试线性部分的反向传播函数。\n",
    "def linear_backward_test_case():\n",
    "    np.random.seed(1)\n",
    "    dZ = np.random.randn(1,2)\n",
    "    A = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "\n",
    "    linear_cache = (A, W, b)\n",
    "\n",
    "    return dZ, linear_cache\n",
    "\n",
    "# 用于测试带激活函数的反向传播。\n",
    "def linear_activation_backward_test_case():\n",
    "    np.random.seed(2)\n",
    "    dA = np.random.randn(1,2) # 当前层的激活梯度 dA，1x2\n",
    "    A = np.random.randn(3,2) # 前一层的激活值，3x2\n",
    "    W = np.random.randn(1,3) # 权重矩阵，1x3\n",
    "    b = np.random.randn(1,1) # 偏置项，1x1\n",
    "    Z = np.dot(W.T, A) + b # 当前层的 Z（线性输出）\n",
    "\n",
    "    activation_cache = (Z) # 激活函数缓存\n",
    "\n",
    "    linear_cache = (A, W, b) # 前向传播缓存\n",
    "    linear_activation_cache = (linear_cache, activation_cache) # 合并缓存\n",
    "\n",
    "    return dA, linear_activation_cache\n",
    "\n",
    "# 生成完整神经网络的反向传播输入，包括前一层和后一层的 cache。\n",
    "def L_model_backward_test_case():\n",
    "    np.random.seed(3)\n",
    "    AL = np.random.randn(1,2) # 最后一层的输出预测值\n",
    "    Y = np.array([[1, 0]]) # 真实标签，只有两个样本\n",
    "\n",
    "    A1 = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    Z1 = np.random.randn(3,2)\n",
    "    linear_cache_activation_1 = ((A1, W1, b1), Z1) # 第一层缓存\n",
    "\n",
    "    A2 = np.random.randn(3, 2)\n",
    "    W2 = np.random.randn(1, 3)\n",
    "    b2 = np.random.randn(1, 1)\n",
    "    Z2 = np.random.randn(1, 2)\n",
    "    linear_cache_activation_2 = ((A2, W2, b2), Z2) # 第二层缓存\n",
    "\n",
    "    caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "    return AL, Y, caches\n",
    "\n",
    "# 生成测试用的参数和梯度，用于测试参数更新函数。\n",
    "def update_parameters_test_case():\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    np.random.seed(3)\n",
    "    dW1 = np.random.randn(3,4)\n",
    "    db1 = np.random.randn(3,1)\n",
    "    dW2 = np.random.randn(1,3)\n",
    "    db2 = np.random.randn(1,1)\n",
    "\n",
    "    grads = {\n",
    "        \"dW1\": dW1,\n",
    "        \"db1\": db1,\n",
    "        \"dW2\": dW2,\n",
    "        \"db2\": db2\n",
    "    }\n",
    "\n",
    "    return parameters, grads"
   ],
   "id": "d282a30ce4aa9575",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 初始化参数\n",
    "### （1）对于一个两层的神经网络结构而言，模型结构是线性->ReLU->线性->sigmod函数。"
   ],
   "id": "ebd0f253875c1bf1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:51:12.850346Z",
     "start_time": "2025-04-07T06:51:12.834008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    此函数是为了初始化两层网络参数而使用的函数。\n",
    "    参数：\n",
    "        n_x - 输入层节点数量\n",
    "        n_h - 隐藏层节点数量\n",
    "        n_y - 输出层节点数量\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含你的参数的python字典：\n",
    "            W1 - 权重矩阵,维度为（n_h，n_x）\n",
    "            b1 - 偏向量，维度为（n_h，1）\n",
    "            W2 - 权重矩阵，维度为（n_y，n_h）\n",
    "            b2 - 偏向量，维度为（n_y，1）\n",
    "\n",
    "    \"\"\"\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    b2 = np.random.randn(n_y, 1)\n",
    "\n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# 测试\n",
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n"
   ],
   "id": "2c55a985a094283e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[ 0.3190391 ]\n",
      " [-0.24937038]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[1.46210794]]\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### （2）初始化多层网络参数",
   "id": "44f9bfa343cec5a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T06:51:15.016821Z",
     "start_time": "2025-04-07T06:51:14.999047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    此函数是为了初始化多层网络参数而使用的函数。\n",
    "    参数：\n",
    "        layers_dims - 包含我们网络中每个图层的节点数量的列表\n",
    "\n",
    "    返回：\n",
    "        parameters - 包含参数“W1”，“b1”，...，“WL”，“bL”的字典：\n",
    "                     W1 - 权重矩阵，维度为（layers_dims [1]，layers_dims [1-1]）\n",
    "                     bl - 偏向量，维度为（layers_dims [1]，1）\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1, L): # 输入层定义为0层，故从1开始\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) / np.sqrt(layer_dims[l-1]) # Xavier 初始化, 缓解梯度爆炸和梯度消失\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        # 断言\n",
    "        assert(parameters[\"W\" + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters[\"b\" + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# 测试\n",
    "layer_dims = [5,4,3,2]\n",
    "print(len(layer_dims))\n",
    "parameters = initialize_parameters_deep(layer_dims)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"W3 = \" + str(parameters[\"W3\"]))\n",
    "print(\"b3 = \" + str(parameters[\"b3\"]))"
   ],
   "id": "2aa6a5eebf2802b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "W1 = [[ 0.79989897  0.19521314  0.04315498 -0.83337927 -0.12405178]\n",
      " [-0.15865304 -0.03700312 -0.28040323 -0.01959608 -0.21341839]\n",
      " [-0.58757818  0.39561516  0.39413741  0.76454432  0.02237573]\n",
      " [-0.18097724 -0.24389238 -0.69160568  0.43932807 -0.49241241]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.59252326 -0.10282495  0.74307418  0.11835813]\n",
      " [-0.51189257 -0.3564966   0.31262248 -0.08025668]\n",
      " [-0.38441818 -0.11501536  0.37252813  0.98805539]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W3 = [[-0.71829494 -0.36166197 -0.46405457]\n",
      " [-1.39665832 -0.53335157 -0.59113495]]\n",
      "b3 = [[0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "- 我们分别构建了两层和多层神经网络的初始化参数的函数，现在我们开始构建前向传播函数。",
   "id": "f329b49a6f949c88"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 前向传播函数\n",
    "前向传播有以下三个步骤:\n",
    "\n",
    "1. LINEAR\n",
    "2. LINEAR - >ACTIVATION，其中激活函数将会使用ReLU或Sigmoid。\n",
    "3. [LINEAR - > RELU] ×（L-1） - > LINEAR - > SIGMOID（整个模型）\n",
    "\n",
    "线性正向传播模块（向量化所有示例）使用公式(3)进行计算：\n",
    "\n",
    "<img src=\"./picture/img_5.png\" width=\"30%\">\n"
   ],
   "id": "a6f6b3514861728e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "反向传播流程：\n",
    "前向： Z1 = W1·X + b1 → A1 = relu(Z1) → Z2 = W2·A1 + b2 → A2 = sigmoid(Z2)\n",
    "\n",
    "反向：\n",
    "\n",
    "       dA2 → dZ2 = dA2 × sigmoid'(Z2)\n",
    "           ↓\n",
    "       dW2, db2, dA1 = linear_backward(dZ2)\n",
    "           ↓\n",
    "       dZ1 = dA1 × relu'(Z1)\n",
    "           ↓\n",
    "       dW1, db1 = linear_backward(dZ1)\n"
   ],
   "id": "61f7727752f3e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性部分【LINEAR】\n",
    "前向传播中，线性部分计算如下："
   ],
   "id": "5db4da5cc8c00a28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:27:51.736677Z",
     "start_time": "2025-04-07T07:27:51.718305Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_forword(A_prev, W, b):\n",
    "    \"\"\"\n",
    "    实现前向传播的线性部分。\n",
    "\n",
    "    参数：\n",
    "        A_prev - 来自上一层（或输入数据）的激活，维度为(上一层的节点数量，示例的数量）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前层的节点数量，前一层的节点数量）\n",
    "        b - 偏向量，numpy向量，维度为（当前层节点数量，1）\n",
    "\n",
    "    返回：\n",
    "         Z - 激活功能的输入，也称为预激活参数\n",
    "         cache - 一个包含“A”，“W”和“b”的字典，存储这些变量以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    assert (Z.shape == (W.shape[0], A.shape[1]))\n",
    "\n",
    "    cache = (A_prev, W, b)\n",
    "\n",
    "    return Z, cache\n",
    "\n",
    "# 测试\n",
    "A_prev, W, b = linear_forward_test_case()\n",
    "Z, linear_cache = linear_forword(A_prev, W, b)\n",
    "print(\"Z = \" + str(Z))\n",
    "print(\"linear_cache = \" + str(linear_cache))\n"
   ],
   "id": "6eab1c2799682ac3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n",
      "linear_cache = (array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ]]), array([[ 1.74481176, -0.7612069 ,  0.3190391 ]]), array([[-0.24937038]]))\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "前向传播的单层计算完成了一半啦！我们来开始构建后半部分",
   "id": "6ff6e1628c7cfec3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 线性激活部分【LINEAR - >ACTIVATION】\n",
    "为了更方便，我们将把两个功能（线性和激活）分组为一个功能（LINEAR-> ACTIVATION）。 因此，我们将实现一个执行LINEAR前进步骤，然后执行ACTIVATION前进步骤的功能。我们来看看这激活函数的数学实现吧~\n",
    "\n",
    "<img src=\"./picture/img_6.png\" width=\"100%\">"
   ],
   "id": "c27ec1527c7cc34a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T07:57:31.424117Z",
     "start_time": "2025-04-07T07:57:31.407340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    实现LINEAR-> ACTIVATION 这一层的前向传播\n",
    "    实现一次完整的“前向传播”过程：线性计算 + 激活函数\n",
    "\n",
    "    参数：\n",
    "        A_prev - 来自上一层（或输入层）的激活，维度为(上一层的节点数量，示例数）\n",
    "        W - 权重矩阵，numpy数组，维度为（当前层的节点数量，前一层的大小）\n",
    "        b - 偏向量，numpy阵列，维度为（当前层的节点数量，1）\n",
    "        activation - 选择在此层中使用的激活函数名，字符串类型，【\"sigmoid\" | \"relu\"】\n",
    "\n",
    "    返回：\n",
    "        A - 激活函数的输出，也称为激活后的值\n",
    "        cache - 一个包含“linear_cache”和“activation_cache”的字典，我们需要存储它以有效地计算后向传递\n",
    "    \"\"\"\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forword(A_prev, W, b) # 拿到上一层的Z以及当前层的权重和偏置\n",
    "        A, activation_cache = sigmoid(Z) # 保存当前层的激活值A以及用于反向传播的Z值\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forword(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "\n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache) # 保存W, b, Z\n",
    "\n",
    "    return A, cache\n",
    "\n",
    "# 测试\n",
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "print(\"A.shape:\" + str(A.shape))\n",
    "print(linear_activation_cache)\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ],
   "id": "9cb318ee09e80526",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "A.shape:(1, 2)\n",
      "((array([[-0.41675785, -0.05626683],\n",
      "       [-2.1361961 ,  1.64027081],\n",
      "       [-1.79343559, -0.84174737]]), array([[ 0.50288142, -1.24528809, -1.05795222]]), array([[-0.90900761]])), array([[ 3.43896131, -2.08938436]]))\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 多层网络的前向传播\n",
    "调用上面的那两个函数来实现，为了在实现L层神经网络时更加方便，我们需要一个函数来复制前一个函数（带有RELU的linear_activation_forward）L-1次，然后用一个带有SIGMOID的linear_activation_forward跟踪它，我们来看一下它的结构是怎样的：\n",
    "<img src=\"./picture/img_7.png\" width=\"100%\">"
   ],
   "id": "674412f615b20fa6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 直观理解：\n",
    "- 整个函数从输入层开始，一层一层计算 Z 和 A，除了最后一层使用 sigmoid，其余使用 relu。\n",
    "\n",
    "- 这些 A 最终传递到输出层，形成预测结果 AL。\n",
    "\n",
    "- 同时把每层的中间数据都保存下来，反向传播时就能用它们计算梯度了。"
   ],
   "id": "3fc99c7d58b133a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:38:04.677550Z",
     "start_time": "2025-04-07T08:38:04.661651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    实现[LINEAR-> RELU] *（L-1） - > LINEAR-> SIGMOID计算前向传播，也就是多层网络的前向传播，为后面每一层都执行LINEAR和ACTIVATION\n",
    "\n",
    "    参数：\n",
    "        X - 数据，numpy数组，维度为（输入节点数量，示例数）\n",
    "        parameters - initialize_parameters_deep（）的输出\n",
    "\n",
    "    返回：\n",
    "        AL - 最后的激活值\n",
    "        caches - 包含以下内容的缓存列表：\n",
    "                 linear_relu_forward（）的每个cache（有L-1个，索引为从0到L-2）\n",
    "                 linear_sigmoid_forward（）的cache（只有一个，索引为L-1）\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "\n",
    "    A = X\n",
    "    L = len(parameters) // 2 # 由于每一层都有 W 和 b，所以参数字典长度是 2L，除以 2 即为总层数。\n",
    "\n",
    "    # 前向传播：[LINEAR -> RELU]*(L-1)\n",
    "    for l in range(1, L): # 循环从第1层到第 L-1 层\n",
    "        A_prev = A # 保存上一层的激活值\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\") # 拿到当前层的激活值和缓存\n",
    "        caches.append(cache) # 把每层的中间值（Z、A、W、b）保存下来，方便反向传播使用。\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\") # AL为最后一层的激活值，即A^[L]，cache为当前层的缓存\n",
    "    caches.append(cache)\n",
    "\n",
    "    # 断言\n",
    "    assert (AL.shape == (1, X.shape[1]))\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "# 测试\n",
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"caches = \" + str(caches))\n"
   ],
   "id": "77bb078c628f7071",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "caches = [((array([[ 1.62434536, -0.61175641],\n",
      "       [-0.52817175, -1.07296862],\n",
      "       [ 0.86540763, -2.3015387 ],\n",
      "       [ 1.74481176, -0.7612069 ]]), array([[ 0.3190391 , -0.24937038,  1.46210794, -2.06014071],\n",
      "       [-0.3224172 , -0.38405435,  1.13376944, -1.09989127],\n",
      "       [-0.17242821, -0.87785842,  0.04221375,  0.58281521]]), array([[-1.10061918],\n",
      "       [ 1.14472371],\n",
      "       [ 0.90159072]])), array([[-2.77991749, -2.82513147],\n",
      "       [-0.11407702, -0.01812665],\n",
      "       [ 2.13860272,  1.40818979]])), ((array([[0.        , 0.        ],\n",
      "       [0.        , 0.        ],\n",
      "       [2.13860272, 1.40818979]]), array([[ 0.50249434,  0.90085595, -0.68372786]]), array([[-0.12289023]])), array([[-1.58511248, -1.08570881]]))]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 计算成本",
   "id": "985386b30bc02152"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T08:41:02.336918Z",
     "start_time": "2025-04-07T08:41:02.330238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    实施成本函数。\n",
    "\n",
    "    参数：\n",
    "        AL - 与标签预测相对应的概率向量，维度为（1，示例数量）\n",
    "        Y - 标签向量（例如：如果不是猫，则为0，如果是猫则为1），维度为（1，数量）\n",
    "\n",
    "    返回：\n",
    "        cost - 交叉熵成本\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -np.sum(np.multiply(np.log(AL), Y) + np.multiply(np.log(1 - AL), 1 - Y)) / m\n",
    "\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost\n",
    "\n",
    "# 测试\n",
    "Y, AL = compute_cost_test_case()\n",
    "cost = compute_cost(AL, Y)\n",
    "print(\"cost = \" + str(cost))"
   ],
   "id": "48f35d56c297a0fc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615397\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 反向传播\n",
    "如图：\n",
    "\n",
    "<img src=\"./picture/img_8.png\" width=\"100%\">\n",
    "\n",
    "对于线性的部分公式：\n",
    "\n",
    "<img src=\"./picture/img_9.png\" width=\"100%\">\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./picture/img_10.png\" width=\"100%\">\n",
    "\n"
   ],
   "id": "6c65160c24f8f9c1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
